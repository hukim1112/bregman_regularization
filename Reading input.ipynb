{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/guide/datasets\n",
    "\n",
    "https://www.tensorflow.org/guide/performance/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading inputs\n",
    "\n",
    "tf.data API를 통해 우리는 on-memory(from_tensor_slice), generator(from_generator), file(TFrecord) 등과 같이 다양한 input source로부터 dataset을 만들고, interleave, map, filter, reduce 등과 같은 함수로 dataset을 재가공 또는 수정하여 최종적으로 iterator를 통해 데이터를 가져오는 방식임을 배웠다.\n",
    "\n",
    "이제 이번 코드에서는 실제 각 case 별로 데이터를 읽어 graph로 가져오는 코드를 수행해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading images in categorical directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_filenames_and_classes(dataset_dir):\n",
    "      flower_root = os.path.join(dataset_dir)\n",
    "      directories = []\n",
    "      class_names = []\n",
    "      for dir_name in os.listdir(flower_root):\n",
    "        path = os.path.join(flower_root, dir_name)\n",
    "        if os.path.isdir(path):\n",
    "          directories.append(path)\n",
    "          class_names.append(dir_name)\n",
    "\n",
    "      photo_filenames = []\n",
    "      for directory in directories:\n",
    "        for filename in os.listdir(directory):\n",
    "          path = os.path.join(directory, filename)\n",
    "          photo_filenames.append(path)\n",
    "\n",
    "      return photo_filenames, sorted(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/dan/prj/datasets/flowers/flower_example1/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths, class_names = _get_filenames_and_classes(path)\n",
    "class_names_to_ids = dict(zip(class_names, range(len(class_names))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = tf.constant('/home/dan/datasets/flower_photos/sunflowers/14901528533_ac1ce09063.jpg')\n",
    "# image_string = tf.read_file(filename)\n",
    "# image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "# image_resized = tf.image.resize_images(image_decoded, [224, 224])\n",
    "# with tf.Session() as sess:\n",
    "#     image = sess.run(image_resized)    \n",
    "    \n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels = 3)\n",
    "    image_resized = tf.image.resize_images(image_decoded, [224, 224])\n",
    "    return image_resized, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "batch_size = 64\n",
    "num_images = len(filepaths)\n",
    "\n",
    "dataset_filepath = tf.data.Dataset.from_tensor_slices(tf.cast(filepaths, tf.string))\n",
    "dataset_class = tf.data.Dataset.from_tensor_slices(\n",
    "    [class_names_to_ids[os.path.basename(os.path.dirname(filepath))] for filepath in filepaths])\n",
    "dataset = tf.data.Dataset.zip((dataset_filepath, dataset_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'IteratorGetNext_1:0' shape=(?, 224, 224, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext_1:1' shape=(?,) dtype=int32>)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(num_images)\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.map(_parse_function, num_parallel_calls=4)\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(2)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "print(next_element)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(1):\n",
    "        a, b = sess.run(next_element)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 224, 224, 3)\n",
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
